model1:
  #1,5 millones
  layer_sizes:
  in_size: 655
  out_size: 11

  training_params:
    episodes: 10_000_000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.06
      epsilon_min: 0.01
      mini_batch_size: 128
      steps_between_optimization: 128
      updates_between_target_sync: 300
      learning_rate_a: 0.0001 #minimum to avoid nan outputs from nn
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 512
      hidden_layer2_size: 1024
      hidden_layer3_size: 512
      hidden_layer4_size: 1024
      output_layer_size: 11
      enable_double_dqn: False

    classification_params:
      reservoir_memory_size: 200000
      mini_batch_size: 128
      steps_between_optimization: 128
      learning_rate_a: 0.1 #min to avoid nan outputs from nn
      input_layer_size: 655
      hidden_layer1_size: 512
      hidden_layer2_size: 1024
      hidden_layer3_size: 512
      hidden_layer4_size: 1024
      output_layer_size: 11

model2:
  #1,5 millones
  layer_sizes:
  in_size: 655
  out_size: 11

  training_params:
    episodes: 100_000_000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.13
      epsilon_min: 0
      mini_batch_size: 256
      steps_between_optimization: 256
      updates_between_target_sync: 1000
      learning_rate_a: 0.0001 #minimum to avoid nan outputs from nn
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512
      output_layer_size: 11
      enable_double_dqn: True

    classification_params:
      reservoir_memory_size: 200000
      mini_batch_size: 256
      steps_between_optimization: 256
      learning_rate_a: 0.05 #changed to 0.01 after error
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512
      output_layer_size: 11

model3:
  #1,5 millones
  training_params:
    episodes: 100_000_000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.13
      epsilon_min: 0
      mini_batch_size: 256
      steps_between_optimization: 256
      updates_between_target_sync: 1000
      learning_rate_a: 0.1 #minimum to avoid nan outputs from nn
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512
      output_layer_size: 11
      enable_double_dqn: True

    classification_params:
      reservoir_memory_size: 200000
      mini_batch_size: 256
      steps_between_optimization: 256
      learning_rate_a: 0.01 #min to avoid nan outputs from nn
      min_prob_of_replacing_reservoir: 0.25
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512
      output_layer_size: 11
    
  exploitability_testing_params:
    episodes: 40000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.0
      epsilon_min: 0
      mini_batch_size: 256
      steps_between_optimization: 256
      updates_between_target_sync: 1000
      learning_rate_a: 0.01 #minimum to avoid nan outputs from nn
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512
      output_layer_size: 11
      enable_double_dqn: True


model4:
  #25 millones
  training_params:
    episodes: 100_000_000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.06
      epsilon_min: 0
      mini_batch_size: 1024
      steps_between_optimization: 256
      updates_between_target_sync: 1000
      learning_rate_a: 0.0001 
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 4096
      hidden_layer2_size: 2048
      hidden_layer3_size: 4096
      hidden_layer4_size: 2048
      output_layer_size: 11
      enable_double_dqn: True

    classification_params:
      reservoir_memory_size: 200000
      mini_batch_size: 1024
      steps_between_optimization: 256
      learning_rate_a: 0.01
      min_prob_of_replacing_reservoir: 0.05 #
      input_layer_size: 655
      hidden_layer1_size: 4096
      hidden_layer2_size: 2048
      hidden_layer3_size: 4096
      hidden_layer4_size: 2048
      output_layer_size: 11

  exploitability_testing_params:
    episodes: 40000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.0
      epsilon_min: 0
      mini_batch_size: 1024
      steps_between_optimization: 1024
      updates_between_target_sync: 1000
      learning_rate_a: 0.01
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 4096
      hidden_layer2_size: 2048
      hidden_layer3_size: 4096
      hidden_layer4_size: 2048
      output_layer_size: 11
      enable_double_dqn: True

model5:
  #2.5 millones | 9 hidden de 512
  training_params:
    episodes: 100_000_000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.13
      epsilon_min: 0
      mini_batch_size: 256
      steps_between_optimization: 256
      updates_between_target_sync: 1000
      learning_rate_a: 0.0001 #minimum to avoid nan outputs from nn
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512 #4 more times
      output_layer_size: 11
      enable_double_dqn: True

    classification_params:
      reservoir_memory_size: 200000
      mini_batch_size: 256
      steps_between_optimization: 256
      learning_rate_a: 0.1 #min to avoid nan outputs from nn
      min_prob_of_replacing_reservoir: 0.05
      input_layer_size: 655
      hidden_layer1_size: 1024
      hidden_layer2_size: 512
      hidden_layer3_size: 1024
      hidden_layer4_size: 512 #4 more times with skip
      output_layer_size: 11

model6:
  training_params:
    episodes: 100_000_000
    dqn_params:
      replay_memory_size: 100000
      epsilon_init: 0.13
      epsilon_min: 0.001
      mini_batch_size: 256
      steps_between_optimization: 256
      updates_between_target_sync: 1000
      learning_rate_a: 0.0001 #minimum to avoid nan outputs from nn
      discount_factor_g: 0.99
      anticipatory_parameter: 0.1 #no cambiar 
      input_layer_size: 655
      hidden_layer1_size: 512
      hidden_layer2_size: 256
      hidden_layer3_size: 128
      hidden_layer4_size: 64
      output_layer_size: 11
      enable_double_dqn: True

    classification_params:
      reservoir_memory_size: 200000
      mini_batch_size: 256
      steps_between_optimization: 256
      learning_rate_a: 0.01 #min to avoid nan outputs from nn
      min_prob_of_replacing_reservoir: 0.05
      input_layer_size: 655
      hidden_layer1_size: 512
      hidden_layer2_size: 256
      hidden_layer3_size: 128
      hidden_layer4_size: 64
      output_layer_size: 11
  
  """
  MODEL5 net
  class DQN(nn.Module):

    def __init__(self, state_dim, action_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4):
        super(DQN, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)
        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)
        self.fc5 = nn.Linear(hidden_dim4, hidden_dim4)
        self.fc6 = nn.Linear(hidden_dim4, hidden_dim4)
        self.fc7 = nn.Linear(hidden_dim4, hidden_dim4)
        self.fc8 = nn.Linear(hidden_dim4, hidden_dim4)
        self.fc9 = nn.Linear(hidden_dim4, hidden_dim4)
        self.output = nn.Linear(hidden_dim4, action_dim)

    def forward(self, x):
        """ x1 = F.relu(self.fc1(x))
        x2 = F.relu(self.fc2(x1))
        x3 = F.relu(self.fc3(x2))
        x4 = F.relu(self.fc4(x3))
        x5 = F.relu(self.fc5(x4))
        x6 = F.relu(self.fc6(x5))
        x7 = F.relu(self.fc7(x6))
        x8 = F.relu(self.fc8(x7))
        x9 = F.relu(self.fc9(x8))
        return self.output(x9) """
        x1 = F.relu(self.fc1(x))
        x2 = F.relu(self.fc2(x1))
        x3 = F.relu(self.fc3(x2))
        x4 = F.relu(self.fc4(x3))
        # res block1
        x5 = F.relu(self.fc5(x4))
        x6 = F.relu(self.fc6(x5)+x4)
        # res block2
        x7 = F.relu(self.fc7(x6))
        x8 = F.relu(self.fc8(x7)+x6)
        #
        x9 = F.relu(self.fc9(x8))
        return self.output(x9) 
  """
